{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL\n",
    "search = \"https://www.linkedin.com/jobs/search/?currentJobId=3743834990&keywords=data%20analyst&origin=SWITCH_SEARCH_VERTICAL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Job Post 1/25\n",
      "Collecting Job Post 2/25\n",
      "Collecting Job Post 3/25\n",
      "Collecting Job Post 4/25\n",
      "Collecting Job Post 5/25\n",
      "Collecting Job Post 6/25\n",
      "Collecting Job Post 7/25\n",
      "Collecting Job Post 8/25\n",
      "Collecting Job Post 9/25\n",
      "Collecting Job Post 10/25\n",
      "Collecting Job Post 11/25\n",
      "Collecting Job Post 12/25\n",
      "Collecting Job Post 13/25\n",
      "Collecting Job Post 14/25\n",
      "Collecting Job Post 15/25\n",
      "Collecting Job Post 16/25\n",
      "Collecting Job Post 17/25\n",
      "Collecting Job Post 18/25\n",
      "Collecting Job Post 19/25\n",
      "Collecting Job Post 20/25\n",
      "Collecting Job Post 21/25\n",
      "Collecting Job Post 22/25\n",
      "Collecting Job Post 23/25\n",
      "Collecting Job Post 24/25\n",
      "Collecting Job Post 25/25\n"
     ]
    }
   ],
   "source": [
    "def getJobsURL(url_search: str , start_page: int = 0, num_pages: int = 1, max_jobs: int = 500):\n",
    "\n",
    "    jobs_url = []\n",
    "    c = 1\n",
    "        \n",
    "    for page in range(0, num_pages):\n",
    "\n",
    "        job_page = num_pages * 25\n",
    "\n",
    "        url = url_search + f'&start={job_page}'\n",
    "\n",
    "        # Request\n",
    "        rq = requests.get(url)\n",
    "        if rq.status_code != 200:\n",
    "            Exception('Invalid Request')\n",
    "\n",
    "        #Parse do html\n",
    "        soup = bs(rq.text, \"html.parser\")\n",
    "\n",
    "        list_href = soup.select('.base-card__full-link')\n",
    "\n",
    "        for i in range(len(list_href)):\n",
    "            jobs_url.append(list_href[i]['href'])\n",
    "            print(f'Collecting Job URL {c}/{num_pages * 25}')\n",
    "            c += 1\n",
    "            time.sleep(0.5)\n",
    "            if c == max_jobs: break\n",
    "\n",
    "    return jobs_url\n",
    "    \n",
    "jobs_url = getJobsURL(search)\n",
    "#print(soup.prettify())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the jobsURL\n",
    "def getJobSoup(job_urls):\n",
    "    soup_jobs = []\n",
    "    c = 1\n",
    "\n",
    "    for i in range(len(job_urls)): \n",
    "        url = job_urls[i]\n",
    "\n",
    "        # Request\n",
    "        rq = requests.get(url)\n",
    "        if rq.status_code != 200:\n",
    "            Exception('Invalid Request')\n",
    "\n",
    "        #Parse do html\n",
    "        soup = bs(rq.text, \"html.parser\")\n",
    "\n",
    "        soup_jobs.append(soup)\n",
    "\n",
    "        print(f'Collecting Job Post {c}/{num_pages * 25}')\n",
    "        c += 1\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    return soup_jobs\n",
    "\n",
    "soup_jobs = getJobSoup(jobs_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@context': 'http://schema.org',\n",
       " '@type': 'JobPosting',\n",
       " 'datePosted': '2023-11-06T00:00:00.000Z',\n",
       " 'description': '&lt;strong&gt;Description&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;strong&gt;About Us: &lt;br&gt;&lt;br&gt;&lt;/strong&gt;Cover Whale is making roads safer by merging insurance solutions with a data-driven driver safety program. Our straightforward insurance options help drivers navigate rising costs while providing solid results for our supporting insurers. We kickstarted our journey in 2019 with a smashing $15.5M seed funding round, the biggest InsurTech seed round in North America up-to-date, and proud to make Forbes\\' list of \"America\\'s Best Startup Employers 2023\". We\\'re growing swiftly, and we\\'d love you to join our journey.&lt;br&gt;&lt;br&gt;&lt;strong&gt;The Role: &lt;br&gt;&lt;br&gt;&lt;/strong&gt;In this role, you will be responsible for extracting, analyzing, and interpreting large datasets to drive insights for the company\\'s programs and operations. The core capability is to acquire data from internal and external sources, move it between our&lt;br&gt;&lt;br&gt;systems, and synthesize it for business reporting and transaction processing to help Cover Whale execute more efficiently and make better decisions.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Responsibilities: &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Collaborate with team members and external data providers to collect and analyze data. &lt;/li&gt;&lt;li&gt;Respond to ad-hoc requests, ensuring accuracy and timely delivery of information. &lt;/li&gt;&lt;li&gt;Establish KPI\\'s to measure the effectiveness of business decisions&lt;/li&gt;&lt;li&gt;Team up with cross-functional teams to identify data requirements to ensure consistent data collection and analysis methodologies. &lt;/li&gt;&lt;li&gt;Identify trends, patterns, and anomalies in data, and provide insights to support business decision-making and strategy development&lt;/li&gt;&lt;li&gt;Stay up-to-date with industry trends and monitor KPI\\'s to identify opportunities for improvement&lt;/li&gt;&lt;li&gt;Create presentations and reports based on recommendations and findings&lt;/li&gt;&lt;li&gt;Conduct ad-hoc analysis of data and provide insights to support strategic decision-making&lt;/li&gt;&lt;li&gt;Engineer new and improve existing data pipelines to facilitate transaction processing and reporting. &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;strong&gt;Compensation:&lt;br&gt;&lt;br&gt;&lt;/strong&gt;The Expected base pay for the role will be between $104,000-$111,000 per year at the commencement of employment. However, base pay if hired will be determined on an individualized basis and is only part of the total compensation package, which, depending on the position, may also include discretionary bonus and other Cover Whale-sponsored total rewards/benefits.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Requirements&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;strong&gt;Education and Experience:&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Bachelor’s degree in a relevant field (Mathematics, Computer Science, Data Analytics, Statistics, etc.) &lt;/li&gt;&lt;li&gt;1-2 years of experience in data analysis, preferably within the insurance industry&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;strong&gt;Required Practical Skills: &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Solid understanding of data ETL, information management, and statistical concepts and principles&lt;/li&gt;&lt;li&gt;Designing and building complex data pipelines in large-scale data environments&lt;/li&gt;&lt;li&gt;Familiarity with developing data products to drive data science and analytics&lt;/li&gt;&lt;li&gt;Experience with ETL and sourcing data from transactional (e.g. insurance policy or claim), financial, and internet of things (e.g. telematics) sources&lt;/li&gt;&lt;li&gt;Video data pipelining and/or analysis a plus&lt;/li&gt;&lt;li&gt;Familiarity with data models, data cleaning, database design, and data mining processes&lt;/li&gt;&lt;li&gt;Practical experience with programming languages like SQL, Python or R, which are used for data manipulation and analysis&lt;/li&gt;&lt;li&gt;Proficiency in data orchestration tools (like Airflow, Dagster, Luigi) for managing and automating complex data workflows effectively&lt;/li&gt;&lt;li&gt;Basic proficiency in database administration&lt;/li&gt;&lt;li&gt;Understanding of data visualization tools (e.g. PowerBI, MetaBase) to present data insights&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;strong&gt;Further Requirements and Professional Abilities: &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Strong analytical thinking and problem-solving skills&lt;/li&gt;&lt;li&gt;Excellent attention to detail&lt;/li&gt;&lt;li&gt;Fundamental knowledge of machine learning is a plus&lt;/li&gt;&lt;li&gt;Practical experience in the application of statistical analysis software can be beneficial&lt;/li&gt;&lt;li&gt;Excellent written and verbal communication skills: the ability to explain findings to others who may not be familiar with the technicalities&lt;/li&gt;&lt;li&gt;A self-starter with the capability to work independently&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;strong&gt;Benefits&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;em&gt;&lt;strong&gt;Perks/Benefits:&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;/em&gt;&lt;ul&gt;&lt;li&gt;Health/Dental/Vision&lt;/li&gt;&lt;li&gt;Life/Disability&lt;/li&gt;&lt;li&gt;Flexible Vacation Policy&lt;/li&gt;&lt;li&gt;401k – Employer Match&lt;/li&gt;&lt;li&gt;Generous Parental Leave&lt;/li&gt;&lt;li&gt;On-Site lunch credit&lt;/li&gt;&lt;li&gt;EAP, Tuition Assistance, Commuter Benefits, and many more. &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;em&gt;Cover Whale works to maintain the best possible environment for our employees, where individuals can learn and grow with the company. We strive to provide a collaborative environment where each person feels encouraged to contribute to their processes, decisions, planning, and culture.&lt;br&gt;&lt;br&gt;&lt;/em&gt;&lt;em&gt;We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.&lt;/em&gt;',\n",
       " 'employmentType': 'FULL_TIME',\n",
       " 'hiringOrganization': {'@type': 'Organization',\n",
       "  'name': 'Cover Whale',\n",
       "  'sameAs': 'https://www.linkedin.com/company/cover-whale',\n",
       "  'logo': 'https://media.licdn.com/dms/image/C4E0BAQGLxHVC92INTg/company-logo_200_200/0/1658331714029/cover_whale_logo?e=2147483647&amp;v=beta&amp;t=UY9ItUI2iw_0E2BjAcuJ-bJuqe87JJ7nm3dfoB1HS5s'},\n",
       " 'identifier': {'@type': 'PropertyValue',\n",
       "  'name': 'Cover Whale',\n",
       "  'value': '63A62D234E'},\n",
       " 'image': 'https://media.licdn.com/dms/image/C4E0BAQGLxHVC92INTg/company-logo_100_100/0/1658331714029/cover_whale_logo?e=2147483647&amp;v=beta&amp;t=mCawdCvakqUyiFr2VqXeTnSgPoQ0xCrQfrIVGKNQfQs',\n",
       " 'industry': 'Technology, Information and Internet,Insurance,Financial Services',\n",
       " 'jobLocation': {'@type': 'Place',\n",
       "  'address': {'@type': 'PostalAddress',\n",
       "   'addressCountry': 'US',\n",
       "   'addressLocality': 'New York',\n",
       "   'addressRegion': 'NY',\n",
       "   'streetAddress': None},\n",
       "  'latitude': 40.713047,\n",
       "  'longitude': -74.00723},\n",
       " 'skills': '',\n",
       " 'title': 'Data Analyst I',\n",
       " 'validThrough': '2023-12-06T16:36:41.000Z',\n",
       " 'educationRequirements': {'@type': 'EducationalOccupationalCredential',\n",
       "  'credentialCategory': 'bachelor degree'},\n",
       " 'experienceRequirements': {'@type': 'OccupationalExperienceRequirements',\n",
       "  'monthsOfExperience': 12}}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getJobAttr(soup_job):\n",
    "\n",
    "    # String JSON\n",
    "    json_string = soup_job.find_all('script', attrs = {'type':'application/ld+json'})[0].text\n",
    "\n",
    "    # Convert a string JSON to Python Object\n",
    "    json_obj = json.loads(json_string)\n",
    "\n",
    "    # Now, the json is a Python dictionary\n",
    "    return json_obj\n",
    "\n",
    "getJobAttr(soup_jobs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jobURL': 'https://www.linkedin.com/jobs/view/data-analyst-i-at-cover-whale-3759423245',\n",
       " 'jobTitle': 'Cover Whale hiring Data Analyst I in New York, New York, United States | LinkedIn',\n",
       " 'jobLocale': 'en_US',\n",
       " 'jobTimePosted': '12:00:00 AM',\n",
       " 'jobCompanyId': '40902095',\n",
       " 'jobIndustryId': '42,6,43'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Metadados\n",
    "def getJobMetaData(soup_job):\n",
    "    postMetaData = {}\n",
    "\n",
    "    postMetaData['jobURL'] = soup_job.find_all('meta', attrs = {'property':'og:url'})[0]['content']\n",
    "    postMetaData['jobTitle'] = soup_job.find('title').text\n",
    "    postMetaData['jobLocale'] = soup_job.find_all('meta', attrs = {'name':'locale'})[0]['content']\n",
    "    postMetaData['jobTimePosted'] = ((soup_job.find_all('meta', attrs = {'name':'description'})[0]['content']).split('.')[0]).replace('Posted ', '')\n",
    "    postMetaData['jobCompanyId'] = soup_job.find_all('meta', attrs = {'name':'companyId'})[0]['content']\n",
    "    postMetaData['jobIndustryId'] = soup_job.find_all('meta', attrs = {'name':'industryIds'})[0]['content']\n",
    "    # postMetaData['jobDescription'] = soup_job.find_all('meta', attrs = {'name':'description'})[0]['content']\n",
    "\n",
    "    return postMetaData\n",
    "\n",
    "getJobMetaData(soup_jobs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boldItens': ['Description',\n",
       "  'About Us: ',\n",
       "  'The Role: ',\n",
       "  'Responsibilities: ',\n",
       "  'Compensation:',\n",
       "  'Requirements',\n",
       "  'Education and Experience:',\n",
       "  'Required Practical Skills: ',\n",
       "  'Further Requirements and Professional Abilities: ',\n",
       "  'Benefits',\n",
       "  'Perks/Benefits:'],\n",
       " 'jobDescription': ' DescriptionAbout Us: Cover Whale is making roads safer by merging insurance solutions with a data-driven driver safety program. Our straightforward insurance options help drivers navigate rising costs while providing solid results for our supporting insurers. We kickstarted our journey in 2019 with a smashing $15.5M seed funding round, the biggest InsurTech seed round in North America up-to-date, and proud to make Forbes\\' list of \"America\\'s Best Startup Employers 2023\". We\\'re growing swiftly, and we\\'d love you to join our journey.The Role: In this role, you will be responsible for extracting, analyzing, and interpreting large datasets to drive insights for the company\\'s programs and operations. The core capability is to acquire data from internal and external sources, move it between oursystems, and synthesize it for business reporting and transaction processing to help Cover Whale execute more efficiently and make better decisions.Responsibilities: Collaborate with team members and external data providers to collect and analyze data. Respond to ad-hoc requests, ensuring accuracy and timely delivery of information. Establish KPI\\'s to measure the effectiveness of business decisionsTeam up with cross-functional teams to identify data requirements to ensure consistent data collection and analysis methodologies. Identify trends, patterns, and anomalies in data, and provide insights to support business decision-making and strategy developmentStay up-to-date with industry trends and monitor KPI\\'s to identify opportunities for improvementCreate presentations and reports based on recommendations and findingsConduct ad-hoc analysis of data and provide insights to support strategic decision-makingEngineer new and improve existing data pipelines to facilitate transaction processing and reporting. Compensation:The Expected base pay for the role will be between $104,000-$111,000 per year at the commencement of employment. However, base pay if hired will be determined on an individualized basis and is only part of the total compensation package, which, depending on the position, may also include discretionary bonus and other Cover Whale-sponsored total rewards/benefits.RequirementsEducation and Experience:Bachelor’s degree in a relevant field (Mathematics, Computer Science, Data Analytics, Statistics, etc.) 1-2 years of experience in data analysis, preferably within the insurance industryRequired Practical Skills: Solid understanding of data ETL, information management, and statistical concepts and principlesDesigning and building complex data pipelines in large-scale data environmentsFamiliarity with developing data products to drive data science and analyticsExperience with ETL and sourcing data from transactional (e.g. insurance policy or claim), financial, and internet of things (e.g. telematics) sourcesVideo data pipelining and/or analysis a plusFamiliarity with data models, data cleaning, database design, and data mining processesPractical experience with programming languages like SQL, Python or R, which are used for data manipulation and analysisProficiency in data orchestration tools (like Airflow, Dagster, Luigi) for managing and automating complex data workflows effectivelyBasic proficiency in database administrationUnderstanding of data visualization tools (e.g. PowerBI, MetaBase) to present data insightsFurther Requirements and Professional Abilities: Strong analytical thinking and problem-solving skillsExcellent attention to detailFundamental knowledge of machine learning is a plusPractical experience in the application of statistical analysis software can be beneficialExcellent written and verbal communication skills: the ability to explain findings to others who may not be familiar with the technicalitiesA self-starter with the capability to work independentlyBenefitsPerks/Benefits:Health/Dental/VisionLife/DisabilityFlexible Vacation Policy401k – Employer MatchGenerous Parental LeaveOn-Site lunch creditEAP, Tuition Assistance, Commuter Benefits, and many more. Cover Whale works to maintain the best possible environment for our employees, where individuals can learn and grow with the company. We strive to provide a collaborative environment where each person feels encouraged to contribute to their processes, decisions, planning, and culture.We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. '}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getFullJobDescription(soup_job):\n",
    "\n",
    "    boldItens = []\n",
    "\n",
    "    for item in range(len(soup_job.select('.show-more-less-html__markup strong'))):\n",
    "        boldItens.append(soup_job.select('.show-more-less-html__markup strong')[item].text)\n",
    "\n",
    "    jobDescription = (soup_job.select('.show-more-less-html__markup')[0].text).replace('\\n', ' ')\n",
    "\n",
    "    fullDescription = {\n",
    "                        'boldItens': boldItens,\n",
    "                        'jobDescription': jobDescription\n",
    "                        }\n",
    "                        \n",
    "    return fullDescription\n",
    "\n",
    "getFullJobDescription(soup_jobs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attributes': {'@context': 'http://schema.org',\n",
       "  '@type': 'JobPosting',\n",
       "  'datePosted': '2023-11-06T00:00:00.000Z',\n",
       "  'description': '&lt;strong&gt;Description&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;strong&gt;About Us: &lt;br&gt;&lt;br&gt;&lt;/strong&gt;Cover Whale is making roads safer by merging insurance solutions with a data-driven driver safety program. Our straightforward insurance options help drivers navigate rising costs while providing solid results for our supporting insurers. We kickstarted our journey in 2019 with a smashing $15.5M seed funding round, the biggest InsurTech seed round in North America up-to-date, and proud to make Forbes\\' list of \"America\\'s Best Startup Employers 2023\". We\\'re growing swiftly, and we\\'d love you to join our journey.&lt;br&gt;&lt;br&gt;&lt;strong&gt;The Role: &lt;br&gt;&lt;br&gt;&lt;/strong&gt;In this role, you will be responsible for extracting, analyzing, and interpreting large datasets to drive insights for the company\\'s programs and operations. The core capability is to acquire data from internal and external sources, move it between our&lt;br&gt;&lt;br&gt;systems, and synthesize it for business reporting and transaction processing to help Cover Whale execute more efficiently and make better decisions.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Responsibilities: &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Collaborate with team members and external data providers to collect and analyze data. &lt;/li&gt;&lt;li&gt;Respond to ad-hoc requests, ensuring accuracy and timely delivery of information. &lt;/li&gt;&lt;li&gt;Establish KPI\\'s to measure the effectiveness of business decisions&lt;/li&gt;&lt;li&gt;Team up with cross-functional teams to identify data requirements to ensure consistent data collection and analysis methodologies. &lt;/li&gt;&lt;li&gt;Identify trends, patterns, and anomalies in data, and provide insights to support business decision-making and strategy development&lt;/li&gt;&lt;li&gt;Stay up-to-date with industry trends and monitor KPI\\'s to identify opportunities for improvement&lt;/li&gt;&lt;li&gt;Create presentations and reports based on recommendations and findings&lt;/li&gt;&lt;li&gt;Conduct ad-hoc analysis of data and provide insights to support strategic decision-making&lt;/li&gt;&lt;li&gt;Engineer new and improve existing data pipelines to facilitate transaction processing and reporting. &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;strong&gt;Compensation:&lt;br&gt;&lt;br&gt;&lt;/strong&gt;The Expected base pay for the role will be between $104,000-$111,000 per year at the commencement of employment. However, base pay if hired will be determined on an individualized basis and is only part of the total compensation package, which, depending on the position, may also include discretionary bonus and other Cover Whale-sponsored total rewards/benefits.&lt;br&gt;&lt;br&gt;&lt;strong&gt;Requirements&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;strong&gt;Education and Experience:&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Bachelor’s degree in a relevant field (Mathematics, Computer Science, Data Analytics, Statistics, etc.) &lt;/li&gt;&lt;li&gt;1-2 years of experience in data analysis, preferably within the insurance industry&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;strong&gt;Required Practical Skills: &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Solid understanding of data ETL, information management, and statistical concepts and principles&lt;/li&gt;&lt;li&gt;Designing and building complex data pipelines in large-scale data environments&lt;/li&gt;&lt;li&gt;Familiarity with developing data products to drive data science and analytics&lt;/li&gt;&lt;li&gt;Experience with ETL and sourcing data from transactional (e.g. insurance policy or claim), financial, and internet of things (e.g. telematics) sources&lt;/li&gt;&lt;li&gt;Video data pipelining and/or analysis a plus&lt;/li&gt;&lt;li&gt;Familiarity with data models, data cleaning, database design, and data mining processes&lt;/li&gt;&lt;li&gt;Practical experience with programming languages like SQL, Python or R, which are used for data manipulation and analysis&lt;/li&gt;&lt;li&gt;Proficiency in data orchestration tools (like Airflow, Dagster, Luigi) for managing and automating complex data workflows effectively&lt;/li&gt;&lt;li&gt;Basic proficiency in database administration&lt;/li&gt;&lt;li&gt;Understanding of data visualization tools (e.g. PowerBI, MetaBase) to present data insights&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;strong&gt;Further Requirements and Professional Abilities: &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;ul&gt;&lt;li&gt;Strong analytical thinking and problem-solving skills&lt;/li&gt;&lt;li&gt;Excellent attention to detail&lt;/li&gt;&lt;li&gt;Fundamental knowledge of machine learning is a plus&lt;/li&gt;&lt;li&gt;Practical experience in the application of statistical analysis software can be beneficial&lt;/li&gt;&lt;li&gt;Excellent written and verbal communication skills: the ability to explain findings to others who may not be familiar with the technicalities&lt;/li&gt;&lt;li&gt;A self-starter with the capability to work independently&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;strong&gt;Benefits&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;em&gt;&lt;strong&gt;Perks/Benefits:&lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/strong&gt;&lt;/em&gt;&lt;ul&gt;&lt;li&gt;Health/Dental/Vision&lt;/li&gt;&lt;li&gt;Life/Disability&lt;/li&gt;&lt;li&gt;Flexible Vacation Policy&lt;/li&gt;&lt;li&gt;401k – Employer Match&lt;/li&gt;&lt;li&gt;Generous Parental Leave&lt;/li&gt;&lt;li&gt;On-Site lunch credit&lt;/li&gt;&lt;li&gt;EAP, Tuition Assistance, Commuter Benefits, and many more. &lt;br&gt;&lt;br&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;em&gt;Cover Whale works to maintain the best possible environment for our employees, where individuals can learn and grow with the company. We strive to provide a collaborative environment where each person feels encouraged to contribute to their processes, decisions, planning, and culture.&lt;br&gt;&lt;br&gt;&lt;/em&gt;&lt;em&gt;We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law.&lt;/em&gt;',\n",
       "  'employmentType': 'FULL_TIME',\n",
       "  'hiringOrganization': {'@type': 'Organization',\n",
       "   'name': 'Cover Whale',\n",
       "   'sameAs': 'https://www.linkedin.com/company/cover-whale',\n",
       "   'logo': 'https://media.licdn.com/dms/image/C4E0BAQGLxHVC92INTg/company-logo_200_200/0/1658331714029/cover_whale_logo?e=2147483647&amp;v=beta&amp;t=UY9ItUI2iw_0E2BjAcuJ-bJuqe87JJ7nm3dfoB1HS5s'},\n",
       "  'identifier': {'@type': 'PropertyValue',\n",
       "   'name': 'Cover Whale',\n",
       "   'value': '63A62D234E'},\n",
       "  'image': 'https://media.licdn.com/dms/image/C4E0BAQGLxHVC92INTg/company-logo_100_100/0/1658331714029/cover_whale_logo?e=2147483647&amp;v=beta&amp;t=mCawdCvakqUyiFr2VqXeTnSgPoQ0xCrQfrIVGKNQfQs',\n",
       "  'industry': 'Technology, Information and Internet,Insurance,Financial Services',\n",
       "  'jobLocation': {'@type': 'Place',\n",
       "   'address': {'@type': 'PostalAddress',\n",
       "    'addressCountry': 'US',\n",
       "    'addressLocality': 'New York',\n",
       "    'addressRegion': 'NY',\n",
       "    'streetAddress': None},\n",
       "   'latitude': 40.713047,\n",
       "   'longitude': -74.00723},\n",
       "  'skills': '',\n",
       "  'title': 'Data Analyst I',\n",
       "  'validThrough': '2023-12-06T16:36:41.000Z',\n",
       "  'educationRequirements': {'@type': 'EducationalOccupationalCredential',\n",
       "   'credentialCategory': 'bachelor degree'},\n",
       "  'experienceRequirements': {'@type': 'OccupationalExperienceRequirements',\n",
       "   'monthsOfExperience': 12}},\n",
       " 'metadata': {'jobURL': 'https://www.linkedin.com/jobs/view/data-analyst-i-at-cover-whale-3759423245',\n",
       "  'jobTitle': 'Cover Whale hiring Data Analyst I in New York, New York, United States | LinkedIn',\n",
       "  'jobLocale': 'en_US',\n",
       "  'jobTimePosted': '12:00:00 AM',\n",
       "  'jobCompanyId': '40902095',\n",
       "  'jobIndustryId': '42,6,43'},\n",
       " 'fullDescription': {'boldItens': ['Description',\n",
       "   'About Us: ',\n",
       "   'The Role: ',\n",
       "   'Responsibilities: ',\n",
       "   'Compensation:',\n",
       "   'Requirements',\n",
       "   'Education and Experience:',\n",
       "   'Required Practical Skills: ',\n",
       "   'Further Requirements and Professional Abilities: ',\n",
       "   'Benefits',\n",
       "   'Perks/Benefits:'],\n",
       "  'jobDescription': ' DescriptionAbout Us: Cover Whale is making roads safer by merging insurance solutions with a data-driven driver safety program. Our straightforward insurance options help drivers navigate rising costs while providing solid results for our supporting insurers. We kickstarted our journey in 2019 with a smashing $15.5M seed funding round, the biggest InsurTech seed round in North America up-to-date, and proud to make Forbes\\' list of \"America\\'s Best Startup Employers 2023\". We\\'re growing swiftly, and we\\'d love you to join our journey.The Role: In this role, you will be responsible for extracting, analyzing, and interpreting large datasets to drive insights for the company\\'s programs and operations. The core capability is to acquire data from internal and external sources, move it between oursystems, and synthesize it for business reporting and transaction processing to help Cover Whale execute more efficiently and make better decisions.Responsibilities: Collaborate with team members and external data providers to collect and analyze data. Respond to ad-hoc requests, ensuring accuracy and timely delivery of information. Establish KPI\\'s to measure the effectiveness of business decisionsTeam up with cross-functional teams to identify data requirements to ensure consistent data collection and analysis methodologies. Identify trends, patterns, and anomalies in data, and provide insights to support business decision-making and strategy developmentStay up-to-date with industry trends and monitor KPI\\'s to identify opportunities for improvementCreate presentations and reports based on recommendations and findingsConduct ad-hoc analysis of data and provide insights to support strategic decision-makingEngineer new and improve existing data pipelines to facilitate transaction processing and reporting. Compensation:The Expected base pay for the role will be between $104,000-$111,000 per year at the commencement of employment. However, base pay if hired will be determined on an individualized basis and is only part of the total compensation package, which, depending on the position, may also include discretionary bonus and other Cover Whale-sponsored total rewards/benefits.RequirementsEducation and Experience:Bachelor’s degree in a relevant field (Mathematics, Computer Science, Data Analytics, Statistics, etc.) 1-2 years of experience in data analysis, preferably within the insurance industryRequired Practical Skills: Solid understanding of data ETL, information management, and statistical concepts and principlesDesigning and building complex data pipelines in large-scale data environmentsFamiliarity with developing data products to drive data science and analyticsExperience with ETL and sourcing data from transactional (e.g. insurance policy or claim), financial, and internet of things (e.g. telematics) sourcesVideo data pipelining and/or analysis a plusFamiliarity with data models, data cleaning, database design, and data mining processesPractical experience with programming languages like SQL, Python or R, which are used for data manipulation and analysisProficiency in data orchestration tools (like Airflow, Dagster, Luigi) for managing and automating complex data workflows effectivelyBasic proficiency in database administrationUnderstanding of data visualization tools (e.g. PowerBI, MetaBase) to present data insightsFurther Requirements and Professional Abilities: Strong analytical thinking and problem-solving skillsExcellent attention to detailFundamental knowledge of machine learning is a plusPractical experience in the application of statistical analysis software can be beneficialExcellent written and verbal communication skills: the ability to explain findings to others who may not be familiar with the technicalitiesA self-starter with the capability to work independentlyBenefitsPerks/Benefits:Health/Dental/VisionLife/DisabilityFlexible Vacation Policy401k – Employer MatchGenerous Parental LeaveOn-Site lunch creditEAP, Tuition Assistance, Commuter Benefits, and many more. Cover Whale works to maintain the best possible environment for our employees, where individuals can learn and grow with the company. We strive to provide a collaborative environment where each person feels encouraged to contribute to their processes, decisions, planning, and culture.We are an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, or any other characteristic protected by law. '}}"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getJobPost(soup_job):\n",
    "    jobPost = {\n",
    "        'attributes': getJobAttr(soup_job),\n",
    "        'metadata': getJobMetaData(soup_job),\n",
    "        'fullDescription': getFullJobDescription(soup_job)\n",
    "    }\n",
    "    return jobPost\n",
    "\n",
    "getJobPost(soup_jobs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Full Script\n",
    "\n",
    "# Imports\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "search = \"https://www.linkedin.com/jobs/search/?currentJobId=3743834990&keywords=data%20analyst&origin=SWITCH_SEARCH_VERTICAL\"\n",
    "\n",
    "jobs_url = getJobsURL(search, num_pages=1)\n",
    "\n",
    "soup_jobs = getJobSoup(jobs_url)\n",
    "\n",
    "jobs_posts = []\n",
    "for job in range(len(soup_jobs)):\n",
    "    jobs_posts.append(getJobPost(soup_jobs[job]))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
